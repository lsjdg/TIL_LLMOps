{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/anaconda3/envs/llm-env/lib/python3.11/site-packages (4.45.2)\n",
      "Requirement already satisfied: datasets==2.21.0 in /opt/anaconda3/envs/llm-env/lib/python3.11/site-packages (2.21.0)\n",
      "Requirement already satisfied: scipy in /opt/anaconda3/envs/llm-env/lib/python3.11/site-packages (1.14.1)\n",
      "Requirement already satisfied: scikit-learn in /opt/anaconda3/envs/llm-env/lib/python3.11/site-packages (1.5.2)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/envs/llm-env/lib/python3.11/site-packages (from datasets==2.21.0) (3.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/envs/llm-env/lib/python3.11/site-packages (from datasets==2.21.0) (1.24.3)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/anaconda3/envs/llm-env/lib/python3.11/site-packages (from datasets==2.21.0) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/anaconda3/envs/llm-env/lib/python3.11/site-packages (from datasets==2.21.0) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/envs/llm-env/lib/python3.11/site-packages (from datasets==2.21.0) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /opt/anaconda3/envs/llm-env/lib/python3.11/site-packages (from datasets==2.21.0) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /opt/anaconda3/envs/llm-env/lib/python3.11/site-packages (from datasets==2.21.0) (4.66.5)\n",
      "Requirement already satisfied: xxhash in /opt/anaconda3/envs/llm-env/lib/python3.11/site-packages (from datasets==2.21.0) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /opt/anaconda3/envs/llm-env/lib/python3.11/site-packages (from datasets==2.21.0) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /opt/anaconda3/envs/llm-env/lib/python3.11/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets==2.21.0) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp in /opt/anaconda3/envs/llm-env/lib/python3.11/site-packages (from datasets==2.21.0) (3.10.10)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.2 in /opt/anaconda3/envs/llm-env/lib/python3.11/site-packages (from datasets==2.21.0) (0.25.2)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/envs/llm-env/lib/python3.11/site-packages (from datasets==2.21.0) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/envs/llm-env/lib/python3.11/site-packages (from datasets==2.21.0) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/envs/llm-env/lib/python3.11/site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/anaconda3/envs/llm-env/lib/python3.11/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /opt/anaconda3/envs/llm-env/lib/python3.11/site-packages (from transformers) (0.20.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/anaconda3/envs/llm-env/lib/python3.11/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/anaconda3/envs/llm-env/lib/python3.11/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/anaconda3/envs/llm-env/lib/python3.11/site-packages (from aiohttp->datasets==2.21.0) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/envs/llm-env/lib/python3.11/site-packages (from aiohttp->datasets==2.21.0) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/envs/llm-env/lib/python3.11/site-packages (from aiohttp->datasets==2.21.0) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/envs/llm-env/lib/python3.11/site-packages (from aiohttp->datasets==2.21.0) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/envs/llm-env/lib/python3.11/site-packages (from aiohttp->datasets==2.21.0) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /opt/anaconda3/envs/llm-env/lib/python3.11/site-packages (from aiohttp->datasets==2.21.0) (1.15.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/envs/llm-env/lib/python3.11/site-packages (from huggingface-hub>=0.21.2->datasets==2.21.0) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/llm-env/lib/python3.11/site-packages (from requests>=2.32.2->datasets==2.21.0) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/llm-env/lib/python3.11/site-packages (from requests>=2.32.2->datasets==2.21.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/llm-env/lib/python3.11/site-packages (from requests>=2.32.2->datasets==2.21.0) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/llm-env/lib/python3.11/site-packages (from requests>=2.32.2->datasets==2.21.0) (2024.8.30)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/envs/llm-env/lib/python3.11/site-packages (from pandas->datasets==2.21.0) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/envs/llm-env/lib/python3.11/site-packages (from pandas->datasets==2.21.0) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/envs/llm-env/lib/python3.11/site-packages (from pandas->datasets==2.21.0) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/llm-env/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets==2.21.0) (1.16.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/anaconda3/envs/llm-env/lib/python3.11/site-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets==2.21.0) (0.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U transformers datasets==2.21.0 scipy scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "task = \"ynat\"  # Yahoo News Article Topic Classification\n",
    "datasets = load_dataset(\"klue\", task)  # Korean Language Understanding Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['guid', 'title', 'label', 'url', 'date'],\n",
       "        num_rows: 45678\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['guid', 'title', 'label', 'url', 'date'],\n",
       "        num_rows: 9107\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'guid': 'ynat-v1_train_00000',\n",
       " 'title': '유튜브 내달 2일까지 크리에이터 지원 공간 운영',\n",
       " 'label': 3,\n",
       " 'url': 'https://news.naver.com/main/read.nhn?mode=LS2D&mid=shm&sid1=105&sid2=227&oid=001&aid=0008508947',\n",
       " 'date': '2016.06.30. 오전 10:36'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "from datasets import ClassLabel\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def show_random_elements(dataset, num_examples=10):\n",
    "    assert num_examples <= len(\n",
    "        dataset\n",
    "    ), \"Can't pick more elements than there are in the dataset.\"\n",
    "\n",
    "    picks = []\n",
    "\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset) - 1)\n",
    "\n",
    "        # 이미 등록된 예제가 뽑힌 경우, 다시 추출\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset) - 1)\n",
    "\n",
    "        picks.append(pick)\n",
    "\n",
    "    # 임의로 추출된 인덱스들로 구성된 데이터 프레임 선언\n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "\n",
    "    for column, typ in dataset.features.items():\n",
    "        # 라벨 클래스를 스트링으로 변환\n",
    "        if isinstance(typ, ClassLabel):\n",
    "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
    "\n",
    "    display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>guid</th>\n",
       "      <th>title</th>\n",
       "      <th>label</th>\n",
       "      <th>url</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ynat-v1_train_16018</td>\n",
       "      <td>中 지난달 차이신 제조업 PMI 49.7…1년반만에 경기위축 진입종합</td>\n",
       "      <td>세계</td>\n",
       "      <td>https://news.naver.com/main/read.nhn?mode=LS2D&amp;mid=shm&amp;sid1=104&amp;sid2=231&amp;oid=001&amp;aid=0010557853</td>\n",
       "      <td>2019.01.02. 오전 11:54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ynat-v1_train_14558</td>\n",
       "      <td>신간 마블이 설계한 사소하고 위대한 과학</td>\n",
       "      <td>생활문화</td>\n",
       "      <td>https://news.naver.com/main/read.nhn?mode=LS2D&amp;mid=shm&amp;sid1=103&amp;sid2=243&amp;oid=001&amp;aid=0011195712</td>\n",
       "      <td>2019.11.07. 오전 10:39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ynat-v1_train_41528</td>\n",
       "      <td>1보 코스피 엿새째 하락 1910선 내줘…코스닥은 2%대 상승</td>\n",
       "      <td>경제</td>\n",
       "      <td>https://news.naver.com/main/read.nhn?mode=LS2D&amp;mid=shm&amp;sid1=101&amp;sid2=258&amp;oid=001&amp;aid=0011007136</td>\n",
       "      <td>2019.08.07. 오후 3:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ynat-v1_train_44837</td>\n",
       "      <td>그래픽 지난해 51채 이상 집부자 1988명</td>\n",
       "      <td>경제</td>\n",
       "      <td>https://news.naver.com/main/read.nhn?mode=LS2D&amp;mid=shm&amp;sid1=101&amp;sid2=263&amp;oid=001&amp;aid=0010542037</td>\n",
       "      <td>2018.12.23. 오후 4:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ynat-v1_train_18671</td>\n",
       "      <td>신간 거짓말 읽는 법·사람의 자리 과학의 마음에 닿다</td>\n",
       "      <td>생활문화</td>\n",
       "      <td>https://news.naver.com/main/read.nhn?mode=LS2D&amp;mid=shm&amp;sid1=103&amp;sid2=243&amp;oid=001&amp;aid=0010773097</td>\n",
       "      <td>2019.04.19. 오전 7:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ynat-v1_train_00507</td>\n",
       "      <td>셰크먼 호기심 분야 꾸준한 연구가 노벨상 비결</td>\n",
       "      <td>IT과학</td>\n",
       "      <td>https://news.naver.com/main/read.nhn?mode=LS2D&amp;mid=shm&amp;sid1=105&amp;sid2=228&amp;oid=001&amp;aid=0008729806</td>\n",
       "      <td>2016.10.05. 오후 2:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ynat-v1_train_09046</td>\n",
       "      <td>IoT로 쓰레기통 관리해요</td>\n",
       "      <td>IT과학</td>\n",
       "      <td>https://news.naver.com/main/read.nhn?mode=LS2D&amp;mid=shm&amp;sid1=105&amp;sid2=227&amp;oid=001&amp;aid=0009270846</td>\n",
       "      <td>2017.05.17. 오전 10:40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ynat-v1_train_42212</td>\n",
       "      <td>금성테크 지에스알파트너스에 의해 파산신청…거래 정지</td>\n",
       "      <td>경제</td>\n",
       "      <td>https://news.naver.com/main/read.nhn?mode=LS2D&amp;mid=shm&amp;sid1=101&amp;sid2=261&amp;oid=001&amp;aid=0008501668</td>\n",
       "      <td>2016.06.27. 오후 5:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ynat-v1_train_42518</td>\n",
       "      <td>게시판 신한금융 기후변화 대응 최우수기업으로 명예의 전당 입성</td>\n",
       "      <td>경제</td>\n",
       "      <td>https://news.naver.com/main/read.nhn?mode=LS2D&amp;mid=shm&amp;sid1=101&amp;sid2=259&amp;oid=001&amp;aid=0010783342</td>\n",
       "      <td>2019.04.24. 오전 11:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ynat-v1_train_35631</td>\n",
       "      <td>카톡 이모티콘 카카오 TV·다음 포털에서도 쓴다</td>\n",
       "      <td>IT과학</td>\n",
       "      <td>https://news.naver.com/main/read.nhn?mode=LS2D&amp;mid=shm&amp;sid1=105&amp;sid2=227&amp;oid=001&amp;aid=0009198040</td>\n",
       "      <td>2017.04.18. 오후 2:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_random_elements(datasets[\"train\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**KLUE TC**\n",
    "\n",
    "- 0 (IT과학)\n",
    "- 1 (경제)\n",
    "- 2 (사회)\n",
    "- 3 (생활문화)\n",
    "- 4 (세계)\n",
    "- 5 (스포츠)\n",
    "- 6 (정치)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# metric 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/h0/f86w2r851zn7yj1w80yw932h0000gn/T/ipykernel_21753/3905583055.py:3: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"f1\", trust_remote_code=True)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "metric = load_metric(\"f1\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0,\n",
       "        1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "        1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1]),\n",
       " array([0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1,\n",
       "        1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1,\n",
       "        1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_preds = np.random.randint(0, 2, size=(64,))\n",
    "fake_labels = np.random.randint(0, 2, size=(64,))\n",
    "\n",
    "fake_preds, fake_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f1': 0.6666666666666666}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric.compute(\n",
    "    predictions=fake_preds,  # 예측 값\n",
    "    references=fake_labels,  # target\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한국어 NLU를 위한 대표적인 모델\n",
    "model_name = \"klue/roberta-base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"프리미어 리그에서 활약할 손흥민 선수의 마지막 한국 인터뷰\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [0, 12416, 4469, 27135, 5943, 2085, 11251, 3825, 2079, 4178, 3629, 5111, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.cls_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='klue/roberta-base', vocab_size=32000, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '[CLS]', 'eos_token': '[SEP]', 'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t3: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t4: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 전처리 함수 정의\n",
    "def preprocess_function(data):\n",
    "    return tokenizer(\n",
    "        data[\"title\"],  # 데이터\n",
    "        truncation=True,  # 문장이 모델이 받아들일 수 있는 최대 길이 이상으로 들어올 경우 최대 길이를 기준으로 시퀀스(문장) 자르기\n",
    "        return_token_type_ids=False,  # token_type_ids가 필요없는 RoBERTa 모델은 False로 설정\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[0, 10637, 8474, 22, 2210, 2299, 2118, 28940, 3691, 4101, 3792, 2], [0, 24905, 1042, 4795, 19982, 2129, 121, 6904, 16311, 3, 14392, 2], [0, 4172, 3797, 3728, 2107, 2134, 3777, 904, 6022, 2332, 2113, 2259, 4523, 1380, 2259, 2062, 2]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_function(datasets[\"train\"][:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모든 데이터에 대해 전처리 적용\n",
    "encoded_datasets = datasets.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "num_labels = 7\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name, num_labels=num_labels\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델이 예측 했을 때 지표 계산\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "    return metric.compute(predictions=predictions, references=labels, average=\"macro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer 정의\n",
    "이제 앞서 정의한 정보들을 바탕으로 `transformers`에서 제공하는 *Trainer* 객체를 활용하기 위한 인자 관리 클래스를 초기화합니다.\n",
    "\n",
    "`metric_name`은 앞서 얻어진 메트릭 함수를 활용했을 때, 아래와 같이 `dict` 형식으로 결과 값이 반환되는데 여기서 우리가 사용할 *key* 를 정의해준다고 생각하시면 됩니다.\n",
    "\n",
    "```python\n",
    ">>> metric.compute(predictions=fake_preds, references=fake_labels)\n",
    "{'f1': 0.85}\n",
    "```\n",
    "\n",
    "각 인자에 대한 자세한 설명은 [문서](https://huggingface.co/transformers/main_classes/trainer.html#trainingarguments)에서 참조해주시면 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate>=0.26.0 in /opt/anaconda3/envs/llm-env/lib/python3.11/site-packages (1.0.1)\n",
      "Requirement already satisfied: transformers[torch] in /opt/anaconda3/envs/llm-env/lib/python3.11/site-packages (4.45.2)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/envs/llm-env/lib/python3.11/site-packages (from transformers[torch]) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/anaconda3/envs/llm-env/lib/python3.11/site-packages (from transformers[torch]) (0.25.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/envs/llm-env/lib/python3.11/site-packages (from transformers[torch]) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/envs/llm-env/lib/python3.11/site-packages (from transformers[torch]) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/envs/llm-env/lib/python3.11/site-packages (from transformers[torch]) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/envs/llm-env/lib/python3.11/site-packages (from transformers[torch]) (2024.9.11)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/envs/llm-env/lib/python3.11/site-packages (from transformers[torch]) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/anaconda3/envs/llm-env/lib/python3.11/site-packages (from transformers[torch]) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /opt/anaconda3/envs/llm-env/lib/python3.11/site-packages (from transformers[torch]) (0.20.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/anaconda3/envs/llm-env/lib/python3.11/site-packages (from transformers[torch]) (4.66.5)\n",
      "Requirement already satisfied: torch in /opt/anaconda3/envs/llm-env/lib/python3.11/site-packages (from transformers[torch]) (2.4.1)\n",
      "Requirement already satisfied: psutil in /opt/anaconda3/envs/llm-env/lib/python3.11/site-packages (from accelerate>=0.26.0) (6.0.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/envs/llm-env/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers[torch]) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/envs/llm-env/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers[torch]) (4.12.2)\n",
      "Requirement already satisfied: sympy in /opt/anaconda3/envs/llm-env/lib/python3.11/site-packages (from torch->transformers[torch]) (1.13.3)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/envs/llm-env/lib/python3.11/site-packages (from torch->transformers[torch]) (3.4.1)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/envs/llm-env/lib/python3.11/site-packages (from torch->transformers[torch]) (3.1.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/llm-env/lib/python3.11/site-packages (from requests->transformers[torch]) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/llm-env/lib/python3.11/site-packages (from requests->transformers[torch]) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/llm-env/lib/python3.11/site-packages (from requests->transformers[torch]) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/llm-env/lib/python3.11/site-packages (from requests->transformers[torch]) (2024.8.30)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/envs/llm-env/lib/python3.11/site-packages (from jinja2->torch->transformers[torch]) (3.0.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/envs/llm-env/lib/python3.11/site-packages (from sympy->torch->transformers[torch]) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install \"transformers[torch]\" \"accelerate>=0.26.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/llm-env/lib/python3.11/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    TrainingArguments,\n",
    ")  # Trainer 클래스에서 사용할 훈련용 하이퍼 파라미터 정의\n",
    "\n",
    "metric_name = \"f1\"  # metric 객체와 동일한 지표를 사용\n",
    "batch_size = 32\n",
    "\n",
    "args = TrainingArguments(\n",
    "    \"../../data/saved_models/test-tc\",  # 모델 결과물을 저장할 디렉토리\n",
    "    evaluation_strategy=\"epoch\",  # 평가 전략. 언제 마다 평가 할지를 결정\n",
    "    save_strategy=\"epoch\",  # 모델 저장은 언제마다 할건지.\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=metric_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "534d3209aa2e4391ab8263b4048ab1e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7140 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5628, 'grad_norm': 9.216388702392578, 'learning_rate': 1.8599439775910366e-05, 'epoch': 0.35}\n",
      "{'loss': 0.39, 'grad_norm': 6.487459182739258, 'learning_rate': 1.719887955182073e-05, 'epoch': 0.7}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41a8c92e28204cbfa01361c12b416432",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/285 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4037039279937744, 'eval_f1': 0.8606807733675929, 'eval_runtime': 33.5434, 'eval_samples_per_second': 271.499, 'eval_steps_per_second': 8.496, 'epoch': 1.0}\n",
      "{'loss': 0.3572, 'grad_norm': 5.403110980987549, 'learning_rate': 1.5798319327731094e-05, 'epoch': 1.05}\n",
      "{'loss': 0.2895, 'grad_norm': 4.995489120483398, 'learning_rate': 1.4397759103641458e-05, 'epoch': 1.4}\n",
      "{'loss': 0.2948, 'grad_norm': 5.401579856872559, 'learning_rate': 1.2997198879551822e-05, 'epoch': 1.75}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e455ef6d0cd5475595cce21d8f7f5b65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/285 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3689662516117096, 'eval_f1': 0.8666661157253167, 'eval_runtime': 37.3849, 'eval_samples_per_second': 243.601, 'eval_steps_per_second': 7.623, 'epoch': 2.0}\n",
      "{'loss': 0.2704, 'grad_norm': 4.673801898956299, 'learning_rate': 1.1596638655462186e-05, 'epoch': 2.1}\n",
      "{'loss': 0.2191, 'grad_norm': 7.442636489868164, 'learning_rate': 1.0196078431372549e-05, 'epoch': 2.45}\n",
      "{'loss': 0.2223, 'grad_norm': 4.694640159606934, 'learning_rate': 8.795518207282914e-06, 'epoch': 2.8}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15710afede284fca9ccd11e606529c0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/285 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4027175009250641, 'eval_f1': 0.8651651509341871, 'eval_runtime': 27.7162, 'eval_samples_per_second': 328.58, 'eval_steps_per_second': 10.283, 'epoch': 3.0}\n",
      "{'loss': 0.2005, 'grad_norm': 7.577108383178711, 'learning_rate': 7.394957983193279e-06, 'epoch': 3.15}\n",
      "{'loss': 0.1614, 'grad_norm': 4.262217998504639, 'learning_rate': 5.994397759103642e-06, 'epoch': 3.5}\n",
      "{'loss': 0.1661, 'grad_norm': 8.207027435302734, 'learning_rate': 4.593837535014006e-06, 'epoch': 3.85}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5ad1aef39c244fe8da135222bbf6042",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/285 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.45816242694854736, 'eval_f1': 0.864923492036132, 'eval_runtime': 34.8216, 'eval_samples_per_second': 261.533, 'eval_steps_per_second': 8.185, 'epoch': 4.0}\n",
      "{'loss': 0.1436, 'grad_norm': 7.979711055755615, 'learning_rate': 3.1932773109243696e-06, 'epoch': 4.2}\n",
      "{'loss': 0.1297, 'grad_norm': 3.0681309700012207, 'learning_rate': 1.792717086834734e-06, 'epoch': 4.55}\n",
      "{'loss': 0.1155, 'grad_norm': 12.434139251708984, 'learning_rate': 3.921568627450981e-07, 'epoch': 4.9}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b950e4ca7d441acbce2af13d52ada0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/285 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.49951910972595215, 'eval_f1': 0.862966249587739, 'eval_runtime': 37.6398, 'eval_samples_per_second': 241.951, 'eval_steps_per_second': 7.572, 'epoch': 5.0}\n",
      "{'train_runtime': 5672.0255, 'train_samples_per_second': 40.266, 'train_steps_per_second': 1.259, 'train_loss': 0.24900020524567248, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=7140, training_loss=0.24900020524567248, metrics={'train_runtime': 5672.0255, 'train_samples_per_second': 40.266, 'train_steps_per_second': 1.259, 'total_flos': 2555331086520900.0, 'train_loss': 0.24900020524567248, 'epoch': 5.0})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer  # TrainingArguments를 입력 받아 훈련을 수행\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,  # 훈련 시킬 모델\n",
    "    args,  # TrainingArguments\n",
    "    train_dataset=encoded_datasets[\"train\"],\n",
    "    eval_dataset=encoded_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "278018225da94cba8cc28945d8f82d1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/285 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.3689662516117096,\n",
       " 'eval_f1': 0.8666661157253167,\n",
       " 'eval_runtime': 39.7928,\n",
       " 'eval_samples_per_second': 228.86,\n",
       " 'eval_steps_per_second': 7.162,\n",
       " 'epoch': 5.0}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 가장 좋은 Metric 을 보인 Model 의 checkpoint\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n",
      "/opt/anaconda3/envs/llm-env/lib/python3.11/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\n",
    "    \"text-classification\",\n",
    "    model=\"../../data/saved_models/test-tc/checkpoint-2856\",\n",
    "    return_all_scores=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "프리미어 리그에서 활약할 손흥민 선수의 마지막 한국 인터뷰\n"
     ]
    }
   ],
   "source": [
    "print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'label': 'LABEL_0', 'score': 0.004838953725993633},\n",
       "  {'label': 'LABEL_1', 'score': 0.0013362442841753364},\n",
       "  {'label': 'LABEL_2', 'score': 0.02681775577366352},\n",
       "  {'label': 'LABEL_3', 'score': 0.7445481419563293},\n",
       "  {'label': 'LABEL_4', 'score': 0.21785353124141693},\n",
       "  {'label': 'LABEL_5', 'score': 0.0016611474566161633},\n",
       "  {'label': 'LABEL_6', 'score': 0.002944271545857191}]]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier(\"기안 84가 갠지스 강물을 마셨습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
